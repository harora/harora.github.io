---
layout: post
title: "Cache-Augmented RAG: Summary"
categories: llm
author: Himanshu Arora
published: yes
---



The world of Large Language Models (LLMs) is constantly evolving, with new techniques emerging to enhance their capabilities. A major way of using LLMs which has gained traction is Retrieval Augmented Generation (RAG). RAG allows LLMs to access and process external information, beyond their limitations of their usual context length. However, RAG still has a major limitation which is the latency involved in retrieving information from external knowledge bases. This is where **Cache-Augmented RAG** [1] promises to help.

## The Bottleneck of Retrieval

Imagine you're asking an LLM to answer a question about a complex scientific paper. Using traditional RAG, the model needs to:

1. **Comprehend your question.**
2. **Search a potentially vast knowledge base for relevant information.**
3. **Retrieve that information.**
4. **Generate a response based on the retrieved information.**

Steps 2 and 3 can be time-consuming, especially when dealing with large or complex knowledge bases. This retrieval latency can significantly impact the overall performance of an LLM application.

## Caching as a Solution

Cache-Augmented RAG addresses this issue by storing the results of previous retrievals in a cache, essentially acting as a **key-value store**.  Think of it as a memory boost for your LLM. When a similar question is asked, the model can bypass the expensive search and retrieval steps, fetching the relevant information directly from the cache. This is similar to how a web browser caches frequently visited websites to load them faster.

## How it Works

The mechanism is quite straightforward:

1. **Embed the query:**  Convert the user's query into a vector representation (embedding) which acts as the **key**.
2. **Search the cache:** Find similar embeddings of previous queries in the cache.
3. **Retrieve and reuse:** If a match is found, retrieve the associated **value** (the previously retrieved information) from the cache. Otherwise, perform the full retrieval process from the knowledge base and update the cache with the new key-value pair (query embedding and retrieved information).

## Benefits of Cache-Augmented RAG

* **Faster response times:**  Reduce retrieval latency and speed up your LLM application.
* **Reduced computational costs:** Fewer retrievals translate to less strain on your knowledge base and lower infrastructure costs.
* **Improved user experience:** Faster responses lead to a smoother and more engaging user experience.


## In Conclusion

Cache-Augmented RAG is a simple yet powerful technique to optimize RAG pipelines. By leveraging a cache as a key-value store, we can significantly improve the performance and efficiency of LLM applications, leading to a better user experience.


[1] Don't Do RAG: When Cache-Augmented Generation is All You Need for Knowledge Tasks
